{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ee1c63",
   "metadata": {},
   "source": [
    "# EEG Motor Movement/Imagery Classification Using Random Forest and Convolutional Neural Networks\n",
    "\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In the following project, I explore the EEG Motor Movement/Imagery Dataset from PhysioNet. The goal is to classify different motor movements and imagery tasks using machine learning techniques, specifically Random Forest and Convolutional Neural Networks (CNNs).\n",
    "\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "> **[EEG Motor Movement/Imagery Dataset](https://physionet.org/content/eegmmidb/1.0.0/)**\n",
    "> \n",
    "> Data set consists of over 1500 one- and two-minute EEG recordings, obtained from 109 volunteers, as described below.\n",
    "> \n",
    "> Subjects performed different motor/imagery tasks while 64-channel EEG were recorded using the BCI2000 system. \n",
    ">\n",
    "> The experimental runs were:\n",
    "> 1. Baseline, eyes open\n",
    "> 2. Baseline, eyes closed\n",
    "> 3. Task 1 (open and close left or right fist)\n",
    "> 4. Task 2 (imagine opening and closing left or right fist)\n",
    "> 5. Task 3 (open and close both fists or both feet)\n",
    "> 6. Task 4 (imagine opening and closing both fists or both feet)\n",
    "> 7. Task 1\n",
    "> 8. Task 2\n",
    "> 9. Task 3\n",
    "> 10. Task 4\n",
    "> 11. Task 1\n",
    "> 12. Task 2\n",
    "> 13. Task 3\n",
    "> 14. Task 4\n",
    "> \n",
    "> Each annotation includes one of three codes (**T0**, **T1**, or **T2**):\n",
    "> - **T0** corresponds to rest\n",
    "> - **T1** corresponds to onset of motion (real or imagined) of\n",
    ">     the left fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    ">     both fists (in runs 5, 6, 9, 10, 13, and 14)\n",
    "> - **T2** corresponds to onset of motion (real or imagined) of\n",
    "    the right fist (in runs 3, 4, 7, 8, 11, and 12)\n",
    "    both feet (in runs 5, 6, 9, 10, 13, and 14)\n",
    "\n",
    "\n",
    "## Development Report\n",
    "\n",
    "### Plan changes\n",
    "\n",
    "Initial intention of the project was to compare performance of Random Forest and CNN model on the EEG Motor Movement/Imagery Dataset. Assumptions were that CNN would outperform Random Forest due to its ability to capture spatial and temporal patterns in the EEG data. However, during the research and implementation phase, I faced models that fit the domain better and provided more reliable results, thus it was decided to add a third instance for comparison: ***.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "EEG data is represented as multichannel time series data with annotations. The following preprocessing steps were applied:\n",
    "\n",
    "- **Bandpass filtering**. Scalp EEG activity shows oscillations at a variety of frequencies. Several of these oscillations have characteristic frequency ranges, spatial distributions and are associated with different states of brain functioning. Alpha (8-12 Hz) and Beta (13-30 Hz) rhythms are particularly relevant for motor imagery tasks. A bandpass filter was applied to retain frequencies between 8-30 Hz, removing irrelevant frequency components and noise.\n",
    "\n",
    "- **Channel selection**. Dataset aquired using 64-channel EEG cap. For motor imagery tasks, channels over frontocentral, central, and centroparietal regions (e.g., FC3, FCz, Cz, CPz) are the most informative. Channels irrelevant to motor imagery were excluded to reduce dimensionality and focus on pertinent signals.\n",
    "\n",
    "- **Epoching**. Continuous EEG data was segmented into fixed-length epochs of 2 seconds aligned to task onsets.\n",
    "\n",
    "<!-- ### Random Forest Model-->\n",
    "\n",
    "<!-- ### CNN -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc624b8",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67dbd7",
   "metadata": {},
   "source": [
    "### Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "INPUT_DIR = os.getenv(\"INPUT_DIR\", \"./data/raw\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./result\")\n",
    "\n",
    "records_file = os.path.join(INPUT_DIR, \"RECORDS\")\n",
    "print(f\"Records file: {records_file}\")\n",
    "\n",
    "SFREQ = 160.0\n",
    "WINDOW_SEC = 2\n",
    "OVERLAP = 0.5\n",
    "\n",
    "FILTER_BANDWIDTH = True\n",
    "MIN_FREQ = 8.0\n",
    "MAX_FREQ = 30.0\n",
    "\n",
    "FILTER_MOTOR_CHANNELS = True\n",
    "# Keeping Frontocentral, Central and Centroparietal channels\n",
    "picks = ['FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6',\n",
    "        'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6',\n",
    "        'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6']\n",
    "\n",
    "CHANNELS = len(picks) if FILTER_MOTOR_CHANNELS else 64\n",
    "\n",
    "EXECUTION_RUNS = [4, 6, 8, 10, 12, 14]\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1099b7e",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "### 1.1. Loading RECORDS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(records_file, \"r\") as f:\n",
    "    records = [line.strip() for line in f if line.strip()]\n",
    "print(f\"Number of RECORDS entries: {len(records)}\")\n",
    "\n",
    "edf_paths = [os.path.join(INPUT_DIR, f\"{record}\") for record in records]\n",
    "print(f\"\\nResolved EDF files: {len(edf_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6255835",
   "metadata": {},
   "source": [
    "### 1.2. EDF file loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "if DEBUG:\n",
    "    mne.set_log_level('DEBUG')\n",
    "else:\n",
    "    mne.set_log_level('ERROR')\n",
    "\n",
    "all_dfs = []\n",
    "for p in edf_paths:\n",
    "    # Read raw EDF file\n",
    "    raw = mne.io.read_raw_edf(p, preload=True, verbose='ERROR')\n",
    "\n",
    "    # Extract subject and run ID\n",
    "    match = re.search(r'S(\\d+)R(\\d+)', p)\n",
    "    if match:\n",
    "        subject_id = int(match.group(1))\n",
    "        run_id = int(match.group(2))\n",
    "    else:\n",
    "        subject_id, run_id = None, None\n",
    "\n",
    "    # Skip if edf only contains T0 (baseline)\n",
    "    events, event_id = mne.events_from_annotations(raw)\n",
    "    active_event_ids = {k: v for k, v in event_id.items() if k != 'T0'}\n",
    "    if not active_event_ids:\n",
    "        continue\n",
    "\n",
    "    # Bandwidth filter, leaving only alpha and beta bands\n",
    "    if FILTER_BANDWIDTH:\n",
    "        raw.filter(MIN_FREQ, MAX_FREQ, fir_design=\"firwin\", skip_by_annotation=\"edge\")\n",
    "\n",
    "    # Filtering channels to relevant ones\n",
    "    if FILTER_MOTOR_CHANNELS:\n",
    "        raw.rename_channels(lambda x: x.strip('.').upper())\n",
    "        raw.pick(picks)\n",
    "\n",
    "    # Resample to target frequency (just in case)\n",
    "    if raw.info['sfreq'] != SFREQ:\n",
    "        raw.resample(SFREQ)\n",
    "        if DEBUG:\n",
    "            print(f\"Resampling from {raw.info['sfreq']} to {SFREQ} Hz: {p}\")\n",
    "    \n",
    "    # Read epochs, filtering baseline\n",
    "    epochs = mne.Epochs(\n",
    "        raw, \n",
    "        events, \n",
    "        event_id=active_event_ids, \n",
    "        tmin=0, \n",
    "        tmax=WINDOW_SEC,\n",
    "        baseline=None, \n",
    "        preload=True, \n",
    "        verbose='ERROR'\n",
    "    )\n",
    "\n",
    "    # Convert to DF and add to our list\n",
    "    df_temp = epochs.to_data_frame().copy()\n",
    "\n",
    "    # Use descriptive strings or Booleans instead of 0/1\n",
    "    df_temp['subject'] = f\"S{subject_id:03d}\"\n",
    "    df_temp['run'] = run_id\n",
    "    df_temp['is_executed'] = run_id in EXECUTION_RUNS\n",
    "    df_temp['task_type'] = 'execution' if run_id in EXECUTION_RUNS else 'imagery'\n",
    "    all_dfs.append(df_temp)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\"Processing file: {raw.info}\")\n",
    "        print(f\"Task label: {raw.annotations.description[0]}\")\n",
    "\n",
    "    del raw\n",
    "    del epochs\n",
    "\n",
    "raws = pd.concat(all_dfs, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
